{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Undertsanding \n",
    "\n",
    "A new business owner wants to open a restuarant in California. He has chosen three cities disctricts to explore.The business owners wants to see the surrounding venues for those cities and what type of restaurants are popular. We will examine the restaurant types and the likes for each type of restaurant. We will see which area what what type of restaurant and the likes for it. \n",
    "\n",
    "\n",
    "# Data\n",
    "\n",
    "Using Foursquare and the raw data scraped from each url. We are only focusing on:\n",
    "\n",
    "- Name\n",
    "- Category \n",
    "- Latitude\n",
    "- Longitude\n",
    "- Id/City\n",
    "\n",
    "Using another API we will get the __likes__ data and applying that to the machine learning models. \n",
    "\n",
    "Combined together we will have an overview of all the information from each city and see the type of restaurant has the most likes. \n",
    "\n",
    "\n",
    "# Methodology \n",
    "\n",
    "\n",
    "The owner wants to predict the 'likes' of a certain type of restaurant. The more likes it can predict the higher chance for success. \n",
    "\n",
    "__Libraries needed:__\n",
    "- Pandas\n",
    "- Numpy \n",
    "- Matplotib \n",
    "- Sci-kit learn\n",
    "\n",
    "__Machine learning models__\n",
    "- Logistic regression: great for predicting classification. In this case __yes__ like or __no__ like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries have been imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np \n",
    "import json\n",
    "import requests\n",
    "from pandas.io.json import json_normalize \n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import folium \n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from geopy.geocoders import Nominatim \n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print('All libraries have been imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The geograpical coordinate of Oakland, California are 37.8044557, -122.2713563.\n",
      "The geograpical coordinate of Emeryville, California are 37.8314089, -122.2865266.\n",
      "The geograpical coordinate of San Diego, California are 32.7174202, -117.1627728.\n"
     ]
    }
   ],
   "source": [
    "address1 = 'Oakland, California'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"foursquare_agent\")\n",
    "location1 = geolocator.geocode(address1)\n",
    "latitude1 = location1.latitude\n",
    "longitude1 = location1.longitude\n",
    "print('The geograpical coordinate of {} are {}, {}.'.format(address1, latitude1, longitude1))\n",
    "\n",
    "address2 = 'Emeryville, California'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"foursquare_agent\")\n",
    "location2 = geolocator.geocode(address2)\n",
    "latitude2 = location2.latitude\n",
    "longitude2 = location2.longitude\n",
    "print('The geograpical coordinate of {} are {}, {}.'.format(address2, latitude2, longitude2))\n",
    "\n",
    "address3 = 'San Diego, California'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"foursquare_agent\")\n",
    "location3 = geolocator.geocode(address3)\n",
    "latitude3 = location3.latitude\n",
    "longitude3 = location3.longitude\n",
    "print('The geograpical coordinate of {} are {}, {}.'.format(address3, latitude3, longitude3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your credentails:\n",
      "CLIENT_ID: FOOE0M5S5QMCVKKXYQZTD2Z55JHNOE0K0Y3SFUXUYTJTLV4Q\n",
      "CLIENT_SECRET:3LUKVDWWTVWBNUEQ5PVDPD5SQB1CBILNUE1HP3WVM44C013M\n",
      "https://api.foursquare.com/v2/venues/explore?&client_id=FOOE0M5S5QMCVKKXYQZTD2Z55JHNOE0K0Y3SFUXUYTJTLV4Q&client_secret=3LUKVDWWTVWBNUEQ5PVDPD5SQB1CBILNUE1HP3WVM44C013M&v=20210804&ll=37.8044557,-122.2713563&radius=1000&limit=100 https://api.foursquare.com/v2/venues/explore?&client_id=FOOE0M5S5QMCVKKXYQZTD2Z55JHNOE0K0Y3SFUXUYTJTLV4Q&client_secret=3LUKVDWWTVWBNUEQ5PVDPD5SQB1CBILNUE1HP3WVM44C013M&v=20210804&ll=37.8314089,-122.2865266&radius=1000&limit=100 https://api.foursquare.com/v2/venues/explore?&client_id=FOOE0M5S5QMCVKKXYQZTD2Z55JHNOE0K0Y3SFUXUYTJTLV4Q&client_secret=3LUKVDWWTVWBNUEQ5PVDPD5SQB1CBILNUE1HP3WVM44C013M&v=20210804&ll=32.7174202,-117.1627728&radius=1000&limit=100\n"
     ]
    }
   ],
   "source": [
    "CLIENT_ID = 'FOOE0M5S5QMCVKKXYQZTD2Z55JHNOE0K0Y3SFUXUYTJTLV4Q' # your Foursquare ID\n",
    "CLIENT_SECRET = '3LUKVDWWTVWBNUEQ5PVDPD5SQB1CBILNUE1HP3WVM44C013M' # your Foursquare Secret\n",
    "VERSION = '20210804' # Foursquare API version\n",
    "\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)\n",
    "\n",
    "\n",
    "LIMIT = 100 # limit of number of venues returned by Foursquare API\n",
    "radius = 1000 # define radius\n",
    "\n",
    "# create URLs\n",
    "url1 = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION, \n",
    "    latitude1, \n",
    "    longitude1, \n",
    "    radius, \n",
    "    LIMIT)\n",
    "\n",
    "\n",
    "# create URLs\n",
    "url2 = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION, \n",
    "    latitude2, \n",
    "    longitude2, \n",
    "    radius, \n",
    "    LIMIT)\n",
    "\n",
    "\n",
    "# create URLs\n",
    "url3 = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION, \n",
    "    latitude3, \n",
    "    longitude3, \n",
    "    radius, \n",
    "    LIMIT)\n",
    "\n",
    "print(url1, url2, url3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-720ff54f978c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# FIRST CITY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mvenues1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'groups'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mnearby_venues1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvenues1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flatten JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'groups'"
     ]
    }
   ],
   "source": [
    "# scrape the data from the generated URLs\n",
    "\n",
    "results1 = requests.get(url1).json()\n",
    "results1\n",
    "\n",
    "results2 = requests.get(url2).json()\n",
    "results2\n",
    "\n",
    "results3 = requests.get(url3).json()\n",
    "results3\n",
    "\n",
    "# function that extracts the category of the venue\n",
    "\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']\n",
    "    \n",
    "\n",
    "# FIRST CITY   \n",
    "\n",
    "venues1 = results1['response']['groups'][0]['items']\n",
    "nearby_venues1 = pd.json_normalize(venues1) # flatten JSON\n",
    "\n",
    "# filter columns\n",
    "filtered_columns1 = ['venue.name', 'venue.categories', 'venue.location.lat', \n",
    "                    'venue.location.lng', 'venue.id']\n",
    "nearby_venues1 = nearby_venues1.loc[:, filtered_columns1]\n",
    "\n",
    "# filter the category for each row\n",
    "nearby_venues1['venue.categories'] = nearby_venues1.apply(get_category_type, axis=1)\n",
    "\n",
    "# clean columns\n",
    "nearby_venues1.columns = [col.split(\".\")[-1] for col in nearby_venues1.columns]\n",
    "\n",
    "# SECOND CITY\n",
    "\n",
    "venues2 = results2['response']['groups'][0]['items']\n",
    "nearby_venues2 = pd.json_normalize(venues2) # flatten JSON\n",
    "\n",
    "# filter columns\n",
    "filtered_columns2 = ['venue.name', 'venue.categories', 'venue.location.lat', \n",
    "                    'venue.location.lng', 'venue.id']\n",
    "nearby_venues2 = nearby_venues2.loc[:, filtered_columns2]\n",
    "\n",
    "# filter the category for each row\n",
    "nearby_venues2['venue.categories'] = nearby_venues2.apply(get_category_type, axis=1)\n",
    "\n",
    "# clean columns\n",
    "nearby_venues2.columns = [col.split(\".\")[-1] for col in nearby_venues2.columns]\n",
    "\n",
    "# THIRD CITY\n",
    "\n",
    "venues3 = results3['response']['groups'][0]['items']\n",
    "nearby_venues3 = pd.json_normalize(venues3) # flatten JSON\n",
    "\n",
    "# filter columns\n",
    "filtered_columns3 = ['venue.name', 'venue.categories', 'venue.location.lat', \n",
    "                    'venue.location.lng', 'venue.id']\n",
    "nearby_venues3 = nearby_venues3.loc[:, filtered_columns3]\n",
    "\n",
    "# filter the category for each row\n",
    "nearby_venues3['venue.categories'] = nearby_venues3.apply(get_category_type, axis=1)\n",
    "\n",
    "# clean columns\n",
    "nearby_venues3.columns = [col.split(\".\")[-1] for col in nearby_venues3.columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('{} venues for Oakland, California were returned by Foursquare.'.format(nearby_venues1.shape[0]))\n",
    "print()\n",
    "print('{} venues for Emeryville, California were returned by Foursquare.'.format(nearby_venues2.shape[0]))\n",
    "print()\n",
    "print('{} venues for San Diego, California were returned by Foursquare.'.format(nearby_venues3.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add locations data to the data sets of each city\n",
    "\n",
    "nearby_venues1['city'] = 'Oakland'\n",
    "nearby_venues2['city'] = 'Emeryville'\n",
    "nearby_venues3['city'] = 'San Diego'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues1.categories.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Oakland: \n",
    "__Top 3 venues__\n",
    "- Bar\n",
    "- Vietnamese restuarant \n",
    "- Chinese restuarant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues2.categories.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Emeryville: Top 3 ranking\n",
    "\n",
    "- Clothing\n",
    "- Coffee\n",
    "- Cosmetic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues3.categories.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**San Diego: Top 3 ranking \n",
    "\n",
    "- Hotel\n",
    "- Italian Restuarant \n",
    "- Mexican "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the three cities into one data set\n",
    "\n",
    "nearby_venues = nearby_venues1.copy()\n",
    "nearby_venues = nearby_venues.append(nearby_venues2)\n",
    "nearby_venues = nearby_venues.append(nearby_venues3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reset the indexing again so the index is in order per city. \n",
    "nearby_venues.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues.categories.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what Restaurants\n",
    "\n",
    "print(df[\"categories\"].value_counts().head(20))\n",
    "plt.figure(figsize=(20,6))\n",
    "sns.countplot(x=\"categories\",data= df,hue= \"\",order = df['categories'].value_counts().index[0:20],palette=\"winter_r\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues['categories'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us focus on restuarants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check list and manually remove all non-restaurant data\n",
    "\n",
    "nearby_venues['categories'].unique()\n",
    "\n",
    "removal_list = ['Clothing Store','Bar','Brewery', \n",
    "                'Comic Shop', 'Yoga Studio','Caf√©', \n",
    "                'Coffee Shop', 'Tiki Bar', 'Music Venue', \n",
    "                'Wine Bar',  'Cocktail Bar', 'Dance Studio', \n",
    "                'Gym / Fitness Center','Beer Bar', \n",
    "                'Bubble Tea Shop', 'Nightclub', 'Food Court', \n",
    "                'Ice Cream Shop', 'Cupcake Shop', 'Skating Rink', \n",
    "                'Dessert Shop', 'Climbing Gym', 'Bakery', \n",
    "                'Farmers Market', 'Gay Bar','Beer Garden',\n",
    "                'Tea Room','Arts & Crafts Store', 'Grocery Store', \n",
    "                'Sports Bar', 'Museum', 'Street Food Gathering', \n",
    "                'Library', 'Skate Park', 'Movie Theater','Park', \n",
    "                'Gym', 'Stadium', 'Furniture / Home Store', 'Discount Store', \n",
    "                'Playground', 'Cosmetics Shop', 'Casino', \n",
    "                'Pet Store','Electronics Store', 'Snack Place',\n",
    "                'Salon / Barbershop', 'Shopping Plaza', 'Deli / Bodega', \n",
    "                'Candy Store', 'Liquor Store', 'Hotel', \n",
    "                'Shoe Store', 'Bookstore', 'Shopping Mall', \n",
    "                'Dive Bar', 'Video Game Store', 'Pharmacy', \n",
    "                'Accessories Store', 'Lingerie Store', 'Mobile Phone Shop', \n",
    "                'Pool Hall', 'Juice Bar', 'Kids Store', \n",
    "                'Supplement Shop', 'Big Box Store', 'Mattress Store', \n",
    "                'Hardware Store', 'Paper / Office Supplies Store', 'Theater', \n",
    "                'Business Service', 'Donut Shop', 'Beer Store', \n",
    "                'Lounge', 'Health Food Store', 'Pedestrian Plaza', \n",
    "                'Hookah Bar', 'Concert Hall', 'Chocolate Shop', \n",
    "                'Hostel', 'Convenience Store', 'Pub', \n",
    "                'Plaza', 'Comedy Club', 'Speakeasy', \n",
    "                'Tattoo Parlor', 'Massage Studio']\n",
    "\n",
    "nearby_venues = nearby_venues[~nearby_venues['categories'].isin(removal_list)]\n",
    "\n",
    "nearby_venues['categories'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will gather data about each type of restuarant based on its number of likes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up to pull the likes from the API based on venue ID\n",
    "\n",
    "url_list = []\n",
    "like_list = []\n",
    "json_list = []\n",
    "\n",
    "for i in list(nearby_venues.id):\n",
    "    venue_url = 'https://api.foursquare.com/v2/venues/{}/likes?client_id={}&client_secret={}&v={}'.format(i, CLIENT_ID, CLIENT_SECRET, VERSION)\n",
    "    url_list.append(venue_url)\n",
    "for link in url_list:\n",
    "    result = requests.get(link).json()\n",
    "    likes = result['response']['likes']['count']\n",
    "    like_list.append(likes)\n",
    "print(like_list)\n",
    "\n",
    "\n",
    "nearby_venues['likes'] = like_list\n",
    "nearby_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = nearby_venues\n",
    "raw_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique restuarants\n",
    "\n",
    "raw_dataset['categories'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can group some cuisines together to make a better categorical variable\n",
    "\n",
    "european = ['Mediterranean Restaurant', 'Scandinavian Restaurant', 'Pizza Place',\n",
    "       'French Restaurant', 'Falafel Restaurant', 'Italian Restaurant',\n",
    "       'Turkish Restaurant']\n",
    "\n",
    "latin = ['Mexican Restaurant', 'Taco Place', 'Brazilian Restaurant', \n",
    "          'Burrito Place']\n",
    "\n",
    "asian = ['Japanese Restaurant', 'Vietnamese Restaurant', 'Chinese Restaurant',\n",
    "         'Hot Dog Joint', 'Hotpot Restaurant', 'Indian Restaurant',\n",
    "         'Thai Restaurant', 'Dumpling Restaurant', 'Dim Sum Restaurant',\n",
    "         'Asian Restaurant', 'Filipino Restaurant', 'Sushi Restaurant',\n",
    "         'Ramen Restaurant']\n",
    "\n",
    "american = ['Vegetarian / Vegan Restaurant', 'Seafood Restaurant', 'Caribbean Restaurant',\n",
    "           'Burger Joint', 'American Restaurant', 'New American Restaurant',\n",
    "            'Southern / Soul Food Restaurant', 'Diner']\n",
    "\n",
    "casual = ['Bagel Shop', 'Sandwich Place', 'Fried Chicken Joint', \n",
    "          'Breakfast Spot', 'Wings Joint', 'Fast Food Restaurant',\n",
    "          'Theme Restaurant']\n",
    "\n",
    "def conditions(s):\n",
    "    if s['categories'] in european:\n",
    "        return 'european'\n",
    "    if s['categories'] in latin:\n",
    "        return 'latin'\n",
    "    if s['categories'] in asian:\n",
    "        return 'asian'\n",
    "    if s['categories'] in american:\n",
    "        return 'american'\n",
    "    if s['categories'] in casual:\n",
    "        return 'casual'\n",
    "\n",
    "raw_dataset['categories_classified'] = raw_dataset.apply(conditions, axis=1)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check to make sure categories_classified has been created correctly\n",
    "\n",
    "pd.crosstab(index = raw_dataset[\"categories_classified\"],\n",
    "            columns=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['likes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to bin for us\n",
    "\n",
    "def rankings(df):\n",
    "    \n",
    "    if df['likes'] <= 60:\n",
    "        return 3\n",
    "    \n",
    "    elif df['likes'] <= 100:\n",
    "        return 2\n",
    "    \n",
    "    elif df['likes'] > 100:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply rankings function to dataset\n",
    "\n",
    "raw_dataset['ranking'] = raw_dataset.apply(rankings, axis=1)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummies for linear regression modelling\n",
    "\n",
    "# one hot encoding\n",
    "reg_dataset = pd.get_dummies(raw_dataset[['categories_classified', \n",
    "                                          'city',]], \n",
    "                               prefix=\"\", \n",
    "                               prefix_sep=\"\")\n",
    "\n",
    "# add name, ranking, and likes columns back to dataframe\n",
    "reg_dataset['ranking'] = raw_dataset['ranking']\n",
    "reg_dataset['likes'] = raw_dataset['likes']\n",
    "reg_dataset['name'] = raw_dataset['name']\n",
    "\n",
    "# move name column to the first column\n",
    "reg_columns = [reg_dataset.columns[-1]] + list(reg_dataset.columns[:-1])\n",
    "reg_dataset = reg_dataset[reg_columns]\n",
    "\n",
    "\n",
    "reg_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "msk = np.random.rand(len(reg_dataset)) < 0.8\n",
    "train = reg_dataset[msk]\n",
    "test = reg_dataset[~msk]\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "x = np.asanyarray(train[['american', 'asian', 'casual',\n",
    "                         'european', 'latin', 'Oakland', \n",
    "                         'Emeryville', 'San Diego']])\n",
    "\n",
    "y = np.asanyarray(train[['likes']])\n",
    "regr.fit (x, y)\n",
    "\n",
    "# The coefficients\n",
    "\n",
    "print ('Coefficients: ', regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression Prediction Capabilities\n",
    "\n",
    "y_hat= regr.predict(test[['american', 'asian', 'casual',\n",
    "                         'european', 'latin', 'Oakland', \n",
    "                         'Emeryville', 'San Diego']])\n",
    "\n",
    "x = np.asanyarray(test[['american', 'asian', 'casual',\n",
    "                         'european', 'latin', 'Oakland', \n",
    "                         'Emeryville', 'San Diego']])\n",
    "\n",
    "y = np.asanyarray(test[['likes']])\n",
    "print(\"Residual sum of squares: %.2f\"\n",
    "      % np.mean((y_hat - y) ** 2))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Ordinal Logistic Regression\n",
    "\n",
    "x_train = np.asanyarray(train[['american', 'asian', 'casual',\n",
    "                         'european', 'latin', 'Oakland', \n",
    "                         'Emeryville', 'San Diego']])\n",
    "\n",
    "y_train = np.asanyarray(train['ranking'])\n",
    "\n",
    "x_test = np.asanyarray(test[['american', 'asian', 'casual',\n",
    "                         'european', 'latin', 'Oakland', \n",
    "                         'Emeryville', 'San Diego']])\n",
    "\n",
    "y_test = np.asanyarray(test['ranking'])\n",
    "\n",
    "\n",
    "mul_ordinal = linear_model.LogisticRegression(multi_class='multinomial',\n",
    "                                              solver='newton-cg',\n",
    "                                              fit_intercept=True).fit(x_train,\n",
    "                                                                      y_train)\n",
    "\n",
    "mul_ordinal\n",
    "\n",
    "coef = mul_ordinal.coef_[0]\n",
    "print (coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Ordinal Logistic Regression Prediction Capabilities\n",
    "\n",
    "yhat = mul_ordinal.predict(x_test)\n",
    "yhat\n",
    "\n",
    "yhat_prob = mul_ordinal.predict_proba(x_test)\n",
    "yhat_prob\n",
    "\n",
    "jaccard_score(y_test, yhat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test, yhat_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of Coefficient Magnitudes of Full Dataset\n",
    "\n",
    "x_all = np.asanyarray(reg_dataset[['american', 'asian', 'casual',\n",
    "                                   'european', 'latin', 'Oakland', \n",
    "                                   'Emeryville', 'San Diego']])\n",
    "\n",
    "y_all = np.asanyarray(reg_dataset['ranking'])\n",
    "\n",
    "\n",
    "\n",
    "LR = linear_model.LogisticRegression(multi_class='multinomial',\n",
    "                                            solver='newton-cg',\n",
    "                                            fit_intercept=True).fit(x_all,\n",
    "                                                                    y_all)\n",
    "\n",
    "LR\n",
    "\n",
    "coef = LR.coef_[0]\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression model was trained on a random subsample of 80% and then the other 20% was used for testing purposes. In order to evaluate if the model is reasonable, the residual sum of squares and variance score were both calculated (10159.10, 0.16). The variance score is quite low, which means that is not a good way of modeling our data. Therefore, we moved on to logistic regression for our analysis. \n",
    "\n",
    "The multinomial ordinal logistic regression model was also trained on a random subsample of 80% and then tested on the remaining 20%. The jaccard score and log-loss were both calculated (26.19% and 1.278 respectively). Although the prediction is not promosing, a jaccard score of 26% is somewhat reasonable. The classification report is included in the analysis. \n",
    "\n",
    "Given the modestly accurate ability of this mode, we have the ability to run the model on the complete dataset. The coefficients we got show that opening a restaurant in Emeryville, or serving cuisine that is asian, or casual, are negatively associated with 'likes'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that given the data, logistic regression presents a better fit for the data over linear regression. Using logistic regression we were able to obtain a Jaccard Score of 26.19%, which although not perfect, is more reasonable than the low variance score obtained from the linear regression. As stated before, please note that for the purposes of this project, we are assumming that likes are a good proxy for how well a new restaurant will do in terms of brand, image and by extension how well the restaurant will perform business-wise. Whether or not these assumptions hold up in a real-life scenario is up for discussion, but this project does contain limitations in scope due to the amount of data that can be fetched from the FourSquare API.\n",
    "\n",
    "As such, to obtain insights into this data, we can proceed with breaking down the results of the logistic regression model. The results showed that the precision score for classifying whether the new restaurant would fall into classes 1, 2, or 3 (highest, medium, lowest) were 40%, 0%, and 50%. Therefore, the model is better at predicting if a restaurant will fall into the best or worst percentile of likes. This is good as we are mostly concerned with whether the restuarant will perform well or not so the high accuracy of predictions for the two extremum is a welcome feature. This allows us to fairly accurately predict the general performance of the business opportunity. Different binning methods for the classes were attempted, but the use of 3 bins by far yielded the best Jaccard Score.\n",
    "\n",
    "Additionally, not only are we attempting to predict the general business performance but also pull insights to inform on business strategy. In this case strategy insight can be gleamed from the coefficient values from running the logistic regression on the full dataset. As such, we can see that opening a restaurant in Emeryville, or serving cuisine that is asian or casual in nature, are associated negatively with \"likes.\" This suggests that the business opportunity should be opening a restaurant in either Oakland or San Diego, with a cuisine that is European, Latin, or American in nature would be the best approach for maximizing likes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing restaurant 'likes' in California from the 300 restaurants, we can conclude that:\n",
    "\n",
    "\n",
    "__3 Best type of restaurants to open__\n",
    "\n",
    "- European \n",
    "- Latino\n",
    "- American \n",
    "\n",
    "__Ranking of 3 cities__\n",
    "\n",
    "- Oakland\n",
    "- San Diego \n",
    "- Emeryville\n",
    "\n",
    "\n",
    "__Data-Driven decision__\n",
    "\n",
    "- The owner would start looking into opening a European restaurant in Oakland. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
